{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pickle\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from statistics import mean\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from impyute.imputation import cs\n",
    "pd.options.display.float_format = '{:20,.15f}'.format\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import copy\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from tensorflow.keras.initializers import GlorotNormal, GlorotUniform, he_normal, he_uniform\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD, Adamax\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_docs as tfdocs\n",
    "import tensorflow_docs.plots\n",
    "import tensorflow_docs.modeling\n",
    "\n",
    "from hyperopt import Trials, STATUS_OK, tpe, rand\n",
    "from hyperas import optim\n",
    "from hyperas.distributions import choice, uniform\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from process import Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "# module references for reload\n",
    "import process_class, output, run_models\n",
    "\n",
    "from process_class import Process\n",
    "from output import output_metrics\n",
    "from run_models import run_nn, run_lgb\n",
    "\n",
    "reload(process_class)\n",
    "reload(output)\n",
    "reload(output)\n",
    "\n",
    "# reimport in case changed\n",
    "from process_class import Process\n",
    "from output import output_metrics\n",
    "from run_models import run_nn, run_lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'datasets'\n",
    "onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "datasets = sorted([d for d in onlyfiles if not 'raw' in d and 'dataset' in d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cols_to_exclude = ['genre__western', 'genre__documentary', 'genre__history', 'country__es', 'country__jp', 'country__ca', 'country__de', 'country__in', 'country__fr', 'country__ru', 'country__it', 'country__au', 'rating__nc-17', 'country__other', 'tag__satire', 'tag__neo_noir', 'tag__sadist', 'tag__cruelty', 'tag__dark', 'tag__storytelling', 'tag__sci_fi', 'tag__psychological', 'tag__absurd', 'tag__philosophical', 'tag__depressing', 'tag__plot_twist', 'tag__realism', 'tag__home_movie', 'tag__thought_provoking']\n",
    "# cols_to_exclude = [col for col in df.columns if (('budget' in col or 'profit' in col) and ('crew' in col or 'cast' in col))]\n",
    "\n",
    "def split_process_df(name, train=0.8, test=0.1):\n",
    "    def get_train_test_revenue(df):\n",
    "        df['revenue'] = df['META__revenue']\n",
    "        dff = df[[col for col in df.columns if not 'META' in col]]\n",
    "        X = dff.drop(['revenue'], axis=1)\n",
    "        y = dff['revenue']\n",
    "        return X, y\n",
    "\n",
    "    df_raw = pd.read_csv(f'datasets/{name}')\n",
    "    # df_raw = reduce_mem_usage(df_raw)\n",
    "    df = shuffle(df_raw, random_state=0)\n",
    "\n",
    "    num_in_train = int(df.shape[0]*0.8)\n",
    "    # num_in_test = int(df.shape[0]*0.1)\n",
    "    num_in_test = int(df.shape[0]*0.1)\n",
    "    df_train = df[:num_in_train].copy()\n",
    "    df_test = df[num_in_train:num_in_train+num_in_test].copy()\n",
    "    df_val = df[num_in_train+num_in_test:].copy()\n",
    "    X_train, y_train = get_train_test_revenue(df_train)\n",
    "    X_test, y_test = get_train_test_revenue(df_test)\n",
    "    X_val, y_val = get_train_test_revenue(df_val)\n",
    "    \n",
    "    data = {}\n",
    "    imputer_func = KNNImputer(n_neighbors=30, weights='distance')\n",
    "    process = Process(X_train, X_test, X_val, y_train, y_test, y_val, imputer='func', imputer_func=imputer_func).skew_X().skew_y().fill_nan()\n",
    "#     process = Process(X_train, X_test, X_val, y_train, y_test, y_val, imputer='knn').skew_X().skew_y().robustscale_Y().fill_nan()\n",
    "#     process = Process(X_train, X_test, X_val, y_train, y_test, y_val, imputer='knn').skew_X().skew_y().robustscale_X().fill_nan()\n",
    "#     process = Process(X_train, X_test, X_val, y_train, y_test, y_val, imputer='knn').robustscale_X().robustscale_Y().skew_X().skew_y().fill_nan()\n",
    "#     process = Process(X_train, X_test, X_val, y_train, y_test, y_val, imputer='knn').skew_X().skew_y().robustscale_X().robustscale_Y().fill_nan()\n",
    "    data['X_train'], data['X_test'], data['X_val'], data['y_train'], data['y_test'], data['y_val'] = process.return_processed()\n",
    "    return data, process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.read_csv(f'datasets/dataset_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_all.columns:\n",
    "    print(f'{df_all[col].isnull().sum()}\\t: {col}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, process = split_process_df('dataset_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_df(name, train=0.8, test=0.1):\n",
    "    def get_train_test_revenue(df):\n",
    "        df['revenue'] = df['META__revenue']\n",
    "        dff = df[[col for col in df.columns if not 'META' in col]]\n",
    "        X = dff.drop(['revenue'], axis=1)\n",
    "        y = dff['revenue']\n",
    "        return X, y\n",
    "\n",
    "    df_raw = pd.read_csv(f'datasets/{name}')\n",
    "    df = shuffle(df_raw, random_state=0)\n",
    "\n",
    "    num_in_train = int(df.shape[0]*0.8)\n",
    "    num_in_test = int(df.shape[0]*0.1)\n",
    "    df_train = df[:num_in_train].copy()\n",
    "    df_test = df[num_in_train:num_in_train+num_in_test].copy()\n",
    "    df_val = df[num_in_train+num_in_test:].copy()\n",
    "    X_train, y_train = get_train_test_revenue(df_train)\n",
    "    X_test, y_test = get_train_test_revenue(df_test)\n",
    "    X_val, y_val = get_train_test_revenue(df_val)\n",
    "    return {\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'X_val': X_val,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'y_val': y_val,\n",
    "    }\n",
    "data = split_df('dataset_all.csv')\n",
    "with open(f'processed/dataset_all_no_process_data.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d_name in datasets:\n",
    "    name = d_name.replace('.csv', '')\n",
    "    print(f'processing {name}')\n",
    "    data, process = split_process_df(d_name, train=0.8, test=0.1)\n",
    "\n",
    "    with open(f'processed/{name}_data.pickle', 'wb') as handle:\n",
    "        pickle.dump(data, handle)\n",
    "\n",
    "    with open(f'processed/{name}_process.pickle', 'wb') as handle:\n",
    "        pickle.dump(process, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/dataset_all_movies_before_data.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    \n",
    "with open('processed/dataset_all_movies_before_process.pickle', 'rb') as handle:\n",
    "    process = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################\n",
    "# Try NN and LGBMR with best features\n",
    "###########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_ranking = {\n",
    "    'features_in_11': [\n",
    "        'budget',\n",
    "        'production_company_1_avg_revenue',\n",
    "        'production_company_2_avg_revenue',\n",
    "        'production_company_3_avg_revenue',\n",
    "        'crew__sound__sound_designer_avg_revenue',\n",
    "        'crew__sound__sound_re_recording_mixer_avg_revenue',\n",
    "        'crew__directing__director__1_avg_revenue',\n",
    "        'crew__production__casting_avg_revenue',\n",
    "        'crew__production__executive_producer__1_avg_revenue',\n",
    "        'crew__production__producer__1_avg_revenue',\n",
    "        'crew__production__producer__2_avg_revenue',\n",
    "        'crew__costume__costume_designer_avg_revenue',\n",
    "        'crew__costume__costume_designer_movies_before',\n",
    "        'crew__costume__costume_supervisor_avg_revenue',\n",
    "        'crew__costume__makeup_artist_avg_revenue',\n",
    "        'crew__art__production_design_avg_revenue',\n",
    "        'crew__art__property_master_avg_revenue',\n",
    "        'crew__camera__director_of_photography_avg_revenue',\n",
    "        'crew__camera__still_photographer_movies_before',\n",
    "        'collection_avg_revenue',\n",
    "        'cast_avg_revenue',  \n",
    "    ],\n",
    "    'features_in_10': [\n",
    "        'production_company_1_avg_profit',\n",
    "        'cast_1_avg_revenue',\n",
    "        'cast_2_avg_revenue',\n",
    "        'cast_3_avg_revenue',\n",
    "        'cast_5_avg_revenue',\n",
    "        'cast_6_avg_revenue',\n",
    "        'cast_7_avg_revenue',\n",
    "        'crew__sound__music_editor_movies_before',\n",
    "        'crew__sound__original_music_composer_avg_profit',\n",
    "        'crew__sound__original_music_composer_avg_revenue',\n",
    "        'crew__sound__original_music_composer_movies_before',\n",
    "        'crew__sound__sound_designer_movies_before',\n",
    "        'crew__sound__sound_effects_editor_avg_revenue',\n",
    "        'crew__sound__sound_re_recording_mixer_movies_before',\n",
    "        'crew__sound__supervising_sound_editor_avg_profit',\n",
    "        'crew__sound__supervising_sound_editor_avg_revenue',\n",
    "        'crew__sound__supervising_sound_editor_movies_before',\n",
    "        'crew__directing__director__1_avg_profit',\n",
    "        'crew__production__casting_avg_profit',\n",
    "        'crew__production__producer__1_avg_profit',\n",
    "        'crew__production__producer__1_movies_before',\n",
    "        'crew__production__producer__2_avg_profit',\n",
    "        'crew__editing__editor__1_avg_profit',\n",
    "        'crew__editing__editor__1_avg_revenue',\n",
    "        'crew__editing__editor__1_movies_before',\n",
    "        'crew__costume__costume_designer_avg_profit',\n",
    "        'crew__costume__costume_supervisor_movies_before',\n",
    "        'crew__crew__stunt_coordinator_avg_revenue',\n",
    "        'crew__writing__screenplay__1_avg_profit',\n",
    "        'crew__writing__screenplay__1_avg_revenue',\n",
    "        'crew__art__art_direction_avg_revenue',\n",
    "        'crew__art__property_master_avg_profit',\n",
    "        'crew__art__property_master_movies_before',\n",
    "        'crew__art__set_decoration_avg_revenue',\n",
    "        'crew__art__set_decoration_movies_before',\n",
    "        'crew__visualeffects__visual_effects_supervisor_avg_profit',\n",
    "        'crew__visualeffects__visual_effects_supervisor_avg_revenue',\n",
    "        'crew__camera__steadicam_operator_avg_revenue',\n",
    "        'crew__camera__steadicam_operator_movies_before',\n",
    "        'collection_avg_profit',\n",
    "        'cast_avg_profit',\n",
    "        'cast_avg_experience',\n",
    "        'cast_avg_movies_before',\n",
    "    ],\n",
    "    'features_in_9': [\n",
    "        'production_company_2_avg_profit',\n",
    "        'production_company_3_avg_profit',\n",
    "        'cast_1_movies_before',\n",
    "        'cast_2_movies_before',\n",
    "        'cast_4_avg_revenue',\n",
    "        'cast_8_avg_revenue',\n",
    "        'crew__sound__sound_designer_avg_profit',\n",
    "        'crew__sound__sound_effects_editor_movies_before',\n",
    "        'crew__directing__script_supervisor_avg_revenue',\n",
    "        'crew__directing__script_supervisor_movies_before',\n",
    "        'crew__production__casting_movies_before',\n",
    "        'crew__production__executive_producer__1_avg_profit',\n",
    "        'crew__production__producer__2_movies_before',\n",
    "        'crew__costume__costume_supervisor_avg_profit',\n",
    "        'crew__costume__makeup_artist_avg_profit',\n",
    "        'crew__costume__makeup_artist_movies_before',\n",
    "        'crew__crew__stunt_coordinator_avg_profit',\n",
    "        'crew__crew__stunt_coordinator_movies_before',\n",
    "        'crew__writing__screenplay__1_movies_before',\n",
    "        'crew__art__production_design_avg_profit',\n",
    "        'crew__art__production_design_movies_before',\n",
    "        'crew__visualeffects__visual_effects_supervisor_movies_before',\n",
    "        'crew__camera__director_of_photography_avg_profit',\n",
    "        'crew__camera__director_of_photography_movies_before',\n",
    "        'crew__camera__steadicam_operator_avg_profit',\n",
    "        'crew__camera__still_photographer_avg_revenue',  \n",
    "    ],\n",
    "    'features_in_8': [\n",
    "        'cast_1_avg_profit',\n",
    "        'cast_1_experience',\n",
    "        'cast_2_avg_profit',\n",
    "        'cast_2_experience',\n",
    "        'cast_3_movies_before',\n",
    "        'cast_4_movies_before',\n",
    "        'cast_5_movies_before',\n",
    "        'cast_6_experience',\n",
    "        'cast_6_movies_before',\n",
    "        'cast_7_movies_before',\n",
    "        'cast_8_movies_before',\n",
    "        'crew__sound__music_editor_avg_revenue',\n",
    "        'crew__sound__sound_re_recording_mixer_avg_profit',\n",
    "        'crew__directing__director__1_movies_before',\n",
    "        'crew__production__executive_producer__1_movies_before',\n",
    "        'crew__art__art_direction_avg_profit',\n",
    "        'crew__art__art_direction_movies_before',\n",
    "        'crew__art__set_decoration_avg_profit',\n",
    "        'crew__camera__still_photographer_avg_profit',\n",
    "    ],\n",
    "    'features_in_7': [\n",
    "        'tag__action',\n",
    "        'homepage_exists',\n",
    "        'cast_3_avg_profit',\n",
    "        'cast_3_experience',\n",
    "        'cast_4_avg_profit',\n",
    "        'cast_4_experience',\n",
    "        'cast_5_experience',\n",
    "        'cast_7_experience',\n",
    "        'cast_8_avg_profit',\n",
    "        'cast_8_experience',\n",
    "        'crew__sound__sound_effects_editor_avg_profit',\n",
    "        'crew__directing__script_supervisor_avg_profit',\n",
    "    ],\n",
    "    'features_in_6': [\n",
    "     'runtime',\n",
    "     'tag__entertaining',\n",
    "     'cast_6_gender',\n",
    "     'cast_5_avg_profit',\n",
    "     'cast_6_avg_profit',\n",
    "     'cast_7_avg_profit',\n",
    "     'crew__sound__music_editor_avg_profit',\n",
    "     'year_avg_revenue',\n",
    "    ],\n",
    "    'features_in_5': [\n",
    "        'spoken_languages',\n",
    "        'genre__adventure',\n",
    "        'genre__comedy',\n",
    "        'genre__horror',\n",
    "        'country__us',\n",
    "        'month_sin',\n",
    "        'competition',\n",
    "        'rating__pg-13',\n",
    "        'rating__r',\n",
    "        'tag__romantic',\n",
    "        'tag__cult',\n",
    "        'tag__comedy',\n",
    "        'tag__humor',\n",
    "        'cast_7_gender',\n",
    "    ],\n",
    "    'features_in_4': [\n",
    "        'weekend',\n",
    "        'genre__romance',\n",
    "        'genre__action',\n",
    "        'country__in',\n",
    "        'country__fr',\n",
    "        'country__gb',\n",
    "        'day_sin',\n",
    "        'day_cos',\n",
    "        'month_cos',\n",
    "        'rating__pg',\n",
    "        'tag__murder',\n",
    "        'tag__violence',\n",
    "        'tag__revenge',\n",
    "        'tag__suspenseful',\n",
    "        'tag__good_versus_evil',\n",
    "        'tag__boring',\n",
    "        'tag__dramatic',\n",
    "        'tag__other',\n",
    "        'cast_1_gender',\n",
    "        'cast_8_gender',\n",
    "    ],\n",
    "    'features_in_3': [\n",
    "        'genre__thriller',\n",
    "        'genre__fantasy',\n",
    "        'tag__flashback',\n",
    "        'tag__psychedelic',\n",
    "        'tag__horror',\n",
    "        'tag__cute',\n",
    "        'cast_2_gender',\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_stay = []\n",
    "for i in [11,10,9,8,7,6,5,4,3]:\n",
    "    print('######################################')\n",
    "    print(f'{i}: lowest features')\n",
    "\n",
    "    with open('processed/dataset_all_data.pickle', 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "\n",
    "    with open('processed/dataset_all_process.pickle', 'rb') as handle:\n",
    "        process = pickle.load(handle)\n",
    "        \n",
    "    features_to_stay.extend(features_ranking[f'features_in_{i}'])\n",
    "    data['X_train'] = data['X_train'][features_to_stay]\n",
    "    data['X_test'] = data['X_test'][features_to_stay]\n",
    "    data['X_val'] = data['X_val'][features_to_stay]\n",
    "    run_lgb()\n",
    "    run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/dataset_all_data.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    \n",
    "with open('processed/dataset_all_process.pickle', 'rb') as handle:\n",
    "    process = pickle.load(handle)\n",
    "\n",
    "features_to_stay = features_in_11 + features_in_10 + features_in_9 + features_in_8\n",
    "data['X_train'] = data['X_train'][features_to_stay]\n",
    "data['X_test'] = data['X_test'][features_to_stay]\n",
    "data['X_val'] = data['X_val'][features_to_stay]\n",
    "run_lgb()\n",
    "run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/dataset_all_data.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    \n",
    "with open('processed/dataset_all_process.pickle', 'rb') as handle:\n",
    "    process = pickle.load(handle)\n",
    "\n",
    "features_to_stay = features_in_11 + features_in_10 + features_in_9\n",
    "data['X_train'] = data['X_train'][features_to_stay]\n",
    "data['X_test'] = data['X_test'][features_to_stay]\n",
    "data['X_val'] = data['X_val'][features_to_stay]\n",
    "run_lgb()\n",
    "run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/dataset_all_data.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    \n",
    "with open('processed/dataset_all_process.pickle', 'rb') as handle:\n",
    "    process = pickle.load(handle)\n",
    "\n",
    "features_to_stay = features_in_11 + features_in_10\n",
    "data['X_train'] = data['X_train'][features_to_stay]\n",
    "data['X_test'] = data['X_test'][features_to_stay]\n",
    "data['X_val'] = data['X_val'][features_to_stay]\n",
    "run_lgb()\n",
    "run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features in 11\n",
    "run_lgb()\n",
    "run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lgb()\n",
    "run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('processed/dataset_all_data.pickle', 'rb') as handle:\n",
    "    data = pickle.load(handle)\n",
    "    \n",
    "with open('processed/dataset_all_process.pickle', 'rb') as handle:\n",
    "    process = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lgb()\n",
    "run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = ['budget',\n",
    " 'runtime',\n",
    " 'spoken_languages',\n",
    " 'weekend',\n",
    " 'genre__romance',\n",
    " 'genre__action',\n",
    " 'genre__thriller',\n",
    " 'genre__comedy',\n",
    " 'genre__horror',\n",
    " 'day_sin',\n",
    " 'day_cos',\n",
    " 'month_sin',\n",
    " 'month_cos',\n",
    " 'competition',\n",
    " 'rating__pg',\n",
    " 'rating__pg-13',\n",
    " 'rating__r',\n",
    " 'tag__murder',\n",
    " 'tag__violence',\n",
    " 'tag__flashback',\n",
    " 'tag__romantic',\n",
    " 'tag__cult',\n",
    " 'tag__revenge',\n",
    " 'tag__comedy',\n",
    " 'tag__suspenseful',\n",
    " 'tag__good_versus_evil',\n",
    " 'tag__humor',\n",
    " 'tag__entertaining',\n",
    " 'tag__action',\n",
    " 'tag__horror',\n",
    " 'tag__dramatic',\n",
    " 'tag__cute',\n",
    " 'tag__other',\n",
    " 'cast_1_gender',\n",
    " 'cast_2_gender',\n",
    " 'cast_6_gender',\n",
    " 'cast_7_gender',\n",
    " 'cast_8_gender',\n",
    " 'homepage_exists',\n",
    " 'production_company_1_avg_profit',\n",
    " 'production_company_1_avg_revenue',\n",
    " 'production_company_2_avg_profit',\n",
    " 'production_company_2_avg_revenue',\n",
    " 'production_company_3_avg_profit',\n",
    " 'production_company_3_avg_revenue',\n",
    " 'cast_1_avg_profit',\n",
    " 'cast_1_experience',\n",
    " 'cast_1_avg_revenue',\n",
    " 'cast_1_movies_before',\n",
    " 'cast_2_avg_profit',\n",
    " 'cast_2_experience',\n",
    " 'cast_2_avg_revenue',\n",
    " 'cast_2_movies_before',\n",
    " 'cast_3_avg_profit',\n",
    " 'cast_3_experience',\n",
    " 'cast_3_avg_revenue',\n",
    " 'cast_3_movies_before',\n",
    " 'cast_4_avg_profit',\n",
    " 'cast_4_experience',\n",
    " 'cast_4_avg_revenue',\n",
    " 'cast_4_movies_before',\n",
    " 'cast_5_avg_profit',\n",
    " 'cast_5_experience',\n",
    " 'cast_5_avg_revenue',\n",
    " 'cast_5_movies_before',\n",
    " 'cast_6_avg_profit',\n",
    " 'cast_6_experience',\n",
    " 'cast_6_avg_revenue',\n",
    " 'cast_6_movies_before',\n",
    " 'cast_7_avg_profit',\n",
    " 'cast_7_experience',\n",
    " 'cast_7_avg_revenue',\n",
    " 'cast_7_movies_before',\n",
    " 'cast_8_avg_profit',\n",
    " 'cast_8_experience',\n",
    " 'cast_8_avg_revenue',\n",
    " 'cast_8_movies_before',\n",
    " 'crew__sound__music_editor_avg_profit',\n",
    " 'crew__sound__music_editor_avg_revenue',\n",
    " 'crew__sound__music_editor_movies_before',\n",
    " 'crew__sound__original_music_composer_avg_profit',\n",
    " 'crew__sound__original_music_composer_avg_revenue',\n",
    " 'crew__sound__original_music_composer_movies_before',\n",
    " 'crew__sound__sound_designer_avg_profit',\n",
    " 'crew__sound__sound_designer_avg_revenue',\n",
    " 'crew__sound__sound_designer_movies_before',\n",
    " 'crew__sound__sound_effects_editor_avg_profit',\n",
    " 'crew__sound__sound_effects_editor_avg_revenue',\n",
    " 'crew__sound__sound_effects_editor_movies_before',\n",
    " 'crew__sound__sound_re_recording_mixer_avg_profit',\n",
    " 'crew__sound__sound_re_recording_mixer_avg_revenue',\n",
    " 'crew__sound__sound_re_recording_mixer_movies_before',\n",
    " 'crew__sound__supervising_sound_editor_avg_profit',\n",
    " 'crew__sound__supervising_sound_editor_avg_revenue',\n",
    " 'crew__sound__supervising_sound_editor_movies_before',\n",
    " 'crew__directing__director__1_avg_profit',\n",
    " 'crew__directing__director__1_avg_revenue',\n",
    " 'crew__directing__director__1_movies_before',\n",
    " 'crew__directing__script_supervisor_avg_profit',\n",
    " 'crew__directing__script_supervisor_avg_revenue',\n",
    " 'crew__directing__script_supervisor_movies_before',\n",
    " 'crew__production__casting_avg_profit',\n",
    " 'crew__production__casting_avg_revenue',\n",
    " 'crew__production__casting_movies_before',\n",
    " 'crew__production__executive_producer__1_avg_profit',\n",
    " 'crew__production__executive_producer__1_avg_revenue',\n",
    " 'crew__production__executive_producer__1_movies_before',\n",
    " 'crew__production__producer__1_avg_profit',\n",
    " 'crew__production__producer__1_avg_revenue',\n",
    " 'crew__production__producer__1_movies_before',\n",
    " 'crew__production__producer__2_avg_profit',\n",
    " 'crew__production__producer__2_avg_revenue',\n",
    " 'crew__production__producer__2_movies_before',\n",
    " 'crew__editing__editor__1_avg_profit',\n",
    " 'crew__editing__editor__1_avg_revenue',\n",
    " 'crew__editing__editor__1_movies_before',\n",
    " 'crew__costume__costume_designer_avg_profit',\n",
    " 'crew__costume__costume_designer_avg_revenue',\n",
    " 'crew__costume__costume_designer_movies_before',\n",
    " 'crew__costume__costume_supervisor_avg_profit',\n",
    " 'crew__costume__costume_supervisor_avg_revenue',\n",
    " 'crew__costume__costume_supervisor_movies_before',\n",
    " 'crew__costume__makeup_artist_avg_profit',\n",
    " 'crew__costume__makeup_artist_avg_revenue',\n",
    " 'crew__costume__makeup_artist_movies_before',\n",
    " 'crew__crew__stunt_coordinator_avg_profit',\n",
    " 'crew__crew__stunt_coordinator_avg_revenue',\n",
    " 'crew__crew__stunt_coordinator_movies_before',\n",
    " 'crew__writing__screenplay__1_avg_profit',\n",
    " 'crew__writing__screenplay__1_avg_revenue',\n",
    " 'crew__writing__screenplay__1_movies_before',\n",
    " 'crew__art__art_direction_avg_profit',\n",
    " 'crew__art__art_direction_avg_revenue',\n",
    " 'crew__art__art_direction_movies_before',\n",
    " 'crew__art__production_design_avg_profit',\n",
    " 'crew__art__production_design_avg_revenue',\n",
    " 'crew__art__production_design_movies_before',\n",
    " 'crew__art__property_master_avg_profit',\n",
    " 'crew__art__property_master_avg_revenue',\n",
    " 'crew__art__property_master_movies_before',\n",
    " 'crew__art__set_decoration_avg_profit',\n",
    " 'crew__art__set_decoration_avg_revenue',\n",
    " 'crew__art__set_decoration_movies_before',\n",
    " 'crew__visualeffects__visual_effects_supervisor_avg_profit',\n",
    " 'crew__visualeffects__visual_effects_supervisor_avg_revenue',\n",
    " 'crew__visualeffects__visual_effects_supervisor_movies_before',\n",
    " 'crew__camera__director_of_photography_avg_profit',\n",
    " 'crew__camera__director_of_photography_avg_revenue',\n",
    " 'crew__camera__director_of_photography_movies_before',\n",
    " 'crew__camera__steadicam_operator_avg_profit',\n",
    " 'crew__camera__steadicam_operator_avg_revenue',\n",
    " 'crew__camera__steadicam_operator_movies_before',\n",
    " 'crew__camera__still_photographer_avg_profit',\n",
    " 'crew__camera__still_photographer_avg_revenue',\n",
    " 'crew__camera__still_photographer_movies_before',\n",
    " 'collection_avg_profit',\n",
    " 'collection_avg_revenue',\n",
    " 'cast_avg_revenue',\n",
    " 'cast_avg_profit',\n",
    " 'cast_avg_experience',\n",
    " 'cast_avg_movies_before',\n",
    " 'year_avg_revenue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['X_train', 'X_test', 'X_val']:\n",
    "    data[i] = data[i][selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_lgb()\n",
    "run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Datasets\t\t\t\t\t\t\tComment\t\t\tNumber of movies\t% from all movies dataset\n",
    "All movies\t\t\t\t\t\t\t\t\t\t7495\t\t\t100\n",
    "US\t\t\t\t\t\t\t\t\t\t\t5695\t\t\t76\n",
    "GB\t\t\t\t\t\t\t\t\t\t\t893\t\t\t12\n",
    "Years 2000-2018\t\t\t\t\t\t\t\t\t\t4903\t\t\t65.4\n",
    "Years 1970-1999\t\t\t\t\t\t\t\t\t\t2042\t\t\t27\n",
    "Profitable movies\t\t\t\t\t\t\t\t\t\t5107\t\t\t68\n",
    "Not profitable movies\t\t\t\t\t\t\t\t\t2335\t\t\t31\n",
    "Budget after 1 percentile \t\t\t\t\tstarts from 8875$\t\t\t7418\t\t\t99\n",
    "Budget after 5 percentile \t\t\t\t\tstarts from 250000$\t\t7129\t\t\t95\n",
    "Profitability 1 percentile cut \t\t\t\tprofitability range [-13.8, 65]\t7342\t\t\t98\n",
    "Profitability 5 percentile cut\t\t\t\t\tprofitability range [-14.9, 13.9]\t6744\t\t\t90\n",
    "Profitability 10 percentile cut\t\t\t\tprofitability range [-4.42, 7.4]\t5994\t\t\t80\n",
    "Profitability 1 percentile cut for movies 2000-2018\t\t\t\t\t\t4814\t\t\t64.2\n",
    "Profitability 1 percentile cut for movies 2000-2018 and budget 1%+\t\t\t\t4754\t\t\t63.4\n",
    "Profitability 1 percentile cut for movies 2000-2018 and budget 5%+\t\t\t\t4599\t\t\t61.3\n",
    "Revenue after 1 percentile\t\t\t\t\tstarts from 10000\t\t\t7354\t\t\t98\n",
    "Revenue after 5 percentile\t\t\t\t\tstarts from 111229\t\t6743\t\t\t90\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'dataset_all.csv'.replace('.csv', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in datasets:\n",
    "    name = dataset.replace('.csv', '')\n",
    "    if name in ['dataset_all', 'dataset_us', 'dataset_gb']:\n",
    "        continue\n",
    "    \n",
    "    with open(f'processed/{name}_data.pickle', 'rb') as handle:\n",
    "        data = pickle.load(handle)\n",
    "    \n",
    "    with open(f'processed/{name}_process.pickle', 'rb') as handle:\n",
    "        process = pickle.load(handle)\n",
    "    \n",
    "    print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')\n",
    "    print('^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^')\n",
    "    print(dataset)\n",
    "    run_lgb()\n",
    "    run_nn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_lgbm(A,P):\n",
    "    return 'smape', smape(A,P), False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for objective in [\n",
    "    'mse', \n",
    "    'mae', \n",
    "    'huber', \n",
    "    'fair', \n",
    "    'poisson', \n",
    "    'quantile', \n",
    "    'mape', \n",
    "]:\n",
    "    print('##################################')\n",
    "    print(objective)\n",
    "    run_lgb(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for loss in [\n",
    "#     'mean_squared_error',\n",
    "#     'mean_absolute_error',\n",
    "#     'mean_absolute_percentage_error',\n",
    "#     'cosine_similarity',\n",
    "#     'huber_loss',\n",
    "    'logcosh',\n",
    "]:\n",
    "    print('##################################')\n",
    "    print(loss)\n",
    "    run_nn(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# fine tune GradientBoostingRegressor\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "random_grid = {\n",
    "    'loss': ['ls', 'lad', 'huber', 'quantile'],\n",
    "    'learning_rate': sp_uniform(loc=0.0001, scale=0.0999),\n",
    "    'n_estimators': [100, 500, 1000],\n",
    "    'subsample': sp_uniform(loc=0.3, scale=0.5),\n",
    "    'criterion': ['friedman_mse', 'mse', 'mae'],\n",
    "    'min_samples_leaf': sp_randint(1, 20),\n",
    "    'min_samples_split': sp_randint(10, 100),\n",
    "    'max_depth': sp_randint(5, 300),\n",
    "    'alpha': sp_uniform(loc=0.85, scale=0.1),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gbr = GradientBoostingRegressor(\n",
    "    random_state=0,\n",
    "    verbose=1,\n",
    "    validation_fraction=0.15,\n",
    "    n_iter_no_change=50,\n",
    ")\n",
    "gbr_random = RandomizedSearchCV(\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    estimator = gbr, \n",
    "    param_distributions = random_grid, \n",
    "    n_iter = 30,\n",
    "    n_jobs=-1,\n",
    "    cv = 3, \n",
    "    refit=True,\n",
    "    verbose=True, \n",
    "    random_state=42, \n",
    ")\n",
    "gbr_random.fit(\n",
    "    data['X_train'], \n",
    "    data['y_train'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DT model selection\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "random_grid = {\n",
    "               'bootstrap': [True, False],\n",
    "               'max_depth': sp_randint(5, 300),\n",
    "               'min_samples_leaf': sp_randint(1, 20),\n",
    "               'min_samples_split': sp_randint(10, 100),\n",
    "               'n_estimators': [100, 500, 1000],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    criterion='mae',\n",
    "    n_jobs=-1,\n",
    "    random_state=0,\n",
    "    verbose=1,\n",
    ")\n",
    "rf_random = RandomizedSearchCV(\n",
    "    scoring='neg_mean_absolute_error',\n",
    "    estimator = rf, \n",
    "    param_distributions = random_grid, \n",
    "    n_iter = 100, \n",
    "    n_jobs=-1,\n",
    "    cv = 3, \n",
    "    refit=True,\n",
    "    verbose=True, \n",
    "    random_state=42, \n",
    ")\n",
    "rf_random.fit(\n",
    "    data['X_train'], \n",
    "    data['y_train'], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################\n",
    "# train AdaBoostRegressor\n",
    "####################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_estimators=40,\n",
    "    max_depth=15,\n",
    "    min_samples_split=0.001,\n",
    "    min_samples_leaf=0.0005,\n",
    "    bootstrap=True,\n",
    "    max_samples=0.95,\n",
    "    criterion='mae', \n",
    "    random_state=0, \n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate' : [0.01,0.05,0.1,0.3,1],\n",
    "    'loss' : ['linear', 'square', 'exponential']\n",
    " }\n",
    "\n",
    "pre_gs_inst = RandomizedSearchCV(AdaBoostRegressor(),\n",
    " param_distributions = param_dist,\n",
    " cv=3,\n",
    " n_iter = 10,\n",
    " n_jobs=-1)\n",
    "\n",
    "pre_gs_inst.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "for loss in ['exponential']:\n",
    "    print(\"##########################\")\n",
    "    print(loss)\n",
    "    abr = AdaBoostRegressor(\n",
    "        base_estimator=model, \n",
    "        n_estimators=50,\n",
    "        random_state=0,\n",
    "        loss=loss,\n",
    "    )\n",
    "    abr.fit(data['X_train'], data['y_train'])\n",
    "    output_metrics(abr, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "\n",
    "for loss in ['square', 'exponential']:\n",
    "    print(\"##########################\")\n",
    "    print(loss)\n",
    "    abr = AdaBoostRegressor(\n",
    "        base_estimator=model, \n",
    "        n_estimators=50,\n",
    "        random_state=0,\n",
    "        loss=loss,\n",
    "    )\n",
    "    abr.fit(data['X_train'], data['y_train'])\n",
    "    output_metrics(abr, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abr.fit(data['X_train'], data['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics(abr, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics(abr, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_010_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_010_decay_power_0995(current_iter):\n",
    "    base_learning_rate = 0.1\n",
    "    lr = base_learning_rate  * np.power(.995, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3\n",
    "\n",
    "def learning_rate_005_decay_power_099(current_iter):\n",
    "    base_learning_rate = 0.05\n",
    "    lr = base_learning_rate  * np.power(.99, current_iter)\n",
    "    return lr if lr > 1e-3 else 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "fit_params={\"early_stopping_rounds\":25, \n",
    "            \"eval_metric\" : 'mse', \n",
    "            \"eval_set\" : [(data['X_test'], data['y_test'])],\n",
    "#             'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n",
    "            'verbose': 100,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test ={\n",
    "    'num_leaves': sp_randint(5, 1000), \n",
    "    'max_depth': sp_randint(5, 100),\n",
    "    'max_bin': sp_randint(100, 1000),\n",
    "    'min_child_samples': sp_randint(100, 400), \n",
    "    'learning_rate': sp_uniform(loc=0.0001, scale=0.0999),\n",
    "#     'num_iterations': [2500, 5000, 7500, 10000],\n",
    "    'min_child_weight': [1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1],\n",
    "    'min_child_samples': sp_randint(100, 400), \n",
    "    'subsample': sp_uniform(loc=0.3, scale=0.5), \n",
    "    'colsample_bytree': sp_uniform(loc=0.6, scale=0.3),\n",
    "    'reg_alpha': sp_uniform(loc=0, scale=5),\n",
    "    'reg_lambda': sp_randint(1, 50),\n",
    "    'bagging_fraction': sp_uniform(loc=0.7, scale=0.3),\n",
    "    'bagging_freq': sp_randint(5, 20),\n",
    "    'feature_fraction': sp_uniform(loc=0.5, scale=0.5),\n",
    "    'min_data_in_leaf': sp_randint(1, 500),\n",
    "    'min_sum_hessian_in_leaf': sp_randint(1, 100),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = lgb.LGBMRegressor(\n",
    "    num_threads=12,\n",
    "    random_state=314, \n",
    "    silent=True, \n",
    "    n_jobs=-1,\n",
    "    verbose=4,\n",
    "    tree_learner='data',\n",
    "# #     num_leaves=\n",
    "#     max_depth=-1,\n",
    "#     learning_rate=0.001,\n",
    "#     num_iterations=100000,\n",
    "#     min_child_weight=10,\n",
    "# #     min_child_samples=\n",
    "#     subsample=0.4,\n",
    "#     colsample_bytree=0.73,\n",
    "#     reg_alpha=3.15,\n",
    "#     reg_lambda=26,\n",
    "# #     max_bin=\n",
    "#     bagging_fraction=0.96,\n",
    "#     bagging_freq=6,\n",
    "#     feature_fraction=0.6,\n",
    "#     min_data_in_leaf=50,\n",
    "#     min_sum_hessian_in_leaf=50,\n",
    ")\n",
    "gs = RandomizedSearchCV(\n",
    "    estimator=clf, \n",
    "    param_distributions=param_test, \n",
    "    n_iter=1000,\n",
    "    cv=3,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(\n",
    "    data['X_train'], \n",
    "    data['y_train'], \n",
    "    **fit_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_lgb = lgb.LGBMRegressor(\n",
    "#     boosting='dart',\n",
    "    num_iterations=50000,\n",
    "    objective='regression',\n",
    "    bagging_fraction=0.937,\n",
    "    bagging_freq=14, \n",
    "    colsample_bytree=0.609,\n",
    "    feature_fraction=0.626,\n",
    "    learning_rate=0.086, \n",
    "    max_bin=117,\n",
    "    max_depth=74,\n",
    "    min_child_samples=169,\n",
    "    min_child_weight=0.01,\n",
    "    min_data_in_leaf=72, \n",
    "    min_sum_hessian_in_leaf=34,\n",
    "#     n_estimators=2500,\n",
    "    num_leaves=411,\n",
    "    reg_alpha=0.5,\n",
    "    reg_lambda=44,\n",
    "    subsample=0.63,\n",
    "    feature_fraction_seed=9,\n",
    "    bagging_seed=9,\n",
    "    tree_learner='data',\n",
    ")\n",
    "mod_lgb.fit(\n",
    "    data['X_train'].values, \n",
    "    data['y_train'],\n",
    "    verbose=2,\n",
    "    eval_metric='rmse',\n",
    "    eval_set=(data['X_test'], data['y_test']),\n",
    "    early_stopping_rounds=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics(mod_lgb, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_lgb = lgb.LGBMRegressor(\n",
    "    objective='regression',\n",
    "    num_leaves=34,\n",
    "    learning_rate=0.001, \n",
    "    n_estimators=7500,\n",
    "    max_bin=192,\n",
    "    max_depth=0,\n",
    "    min_child_samples=160,\n",
    "    min_child_weight=0.001,\n",
    "    bagging_fraction=0.98,\n",
    "    bagging_freq=15, \n",
    "    feature_fraction=0.77,\n",
    "    metric='l2',\n",
    "    bagging_seed=9,\n",
    "    min_data_in_leaf=1, \n",
    "    min_sum_hessian_in_leaf=50,\n",
    "    colsample_bytree=0.87,\n",
    "    reg_alpha=0.18,\n",
    "    reg_lambda=30,\n",
    "    subsample=0.39,\n",
    "    tree_learner='data',\n",
    ")\n",
    "mod_lgb.fit(\n",
    "    data['X_train'].values, \n",
    "    data['y_train'],\n",
    "    verbose=2,\n",
    "    eval_metric='mse',\n",
    "    eval_set=[(data['X_test'], data['y_test'])],\n",
    "    early_stopping_rounds=100\n",
    ")\n",
    "output_metrics(mod_lgb, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test_etr ={\n",
    "    'n_estimators': [100, 500, 750, 1000, 5000, 10000],\n",
    "    'max_depth': sp_randint(1, 200),\n",
    "    'max_features': sp_randint(10, 225), \n",
    "    'min_samples_leaf': sp_randint(1,50), \n",
    "    'min_samples_split': sp_randint(1,50),\n",
    "    'min_weight_fraction_leaf': sp_uniform(0.0, 0.5),\n",
    "    'max_leaf_nodes': sp_randint(1, 200),\n",
    "    'min_impurity_decrease': sp_uniform(0.0, 5),\n",
    "    'ccp_alpha': sp_uniform(0.0, 5),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_etr = ExtraTreesRegressor(\n",
    "    n_jobs=-1, \n",
    "    random_state=0,\n",
    "    verbose=5,\n",
    ")\n",
    "reg_etr_gs = RandomizedSearchCV(\n",
    "    n_jobs=-1, \n",
    "    estimator=reg_etr, \n",
    "    param_distributions=param_test_etr, \n",
    "    n_iter=10,\n",
    "    cv=5,\n",
    "    refit=True,\n",
    "    random_state=314,\n",
    "    verbose=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "reg_etr_gs.fit(data['X_train'], data['y_train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best score reached: {} with params: {} '.format(reg_etr_gs.best_score_, reg_etr_gs.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etr = ExtraTreesRegressor(\n",
    "    n_jobs=-1, \n",
    "    ccp_alpha=0.32,\n",
    "    max_depth=10,\n",
    "    max_features=71, \n",
    "    max_leaf_nodes=55,\n",
    "    min_impurity_decrease=0.35,\n",
    "    min_samples_leaf=8, \n",
    "    min_samples_split=23,\n",
    "    min_weight_fraction_leaf=0.29,\n",
    "    n_estimators=5000, \n",
    "    \n",
    ")\n",
    "etr.fit(data['X_train'].values, data['y_train'])\n",
    "output_metrics(etr, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(Dense(\n",
    "        {{choice([128, 192, 256, 512, 1024, 2048])}}, \n",
    "        input_shape=[X_train.shape[1]],\n",
    "        kernel_initializer={{choice(['zeros', 'glorot_normal', 'he_normal'])}},           \n",
    "        bias_initializer={{choice(['zeros', 'glorot_normal', 'he_normal'])}},  \n",
    "        kernel_regularizer=l1_l2(\n",
    "            l1={{uniform(0, 0.1)}}, \n",
    "            l2={{uniform(0, 0.1)}}),\n",
    "        bias_regularizer=l1_l2(\n",
    "            l1={{uniform(0, 0.1)}}, \n",
    "            l2={{uniform(0, 0.1)}}),\n",
    "        activity_regularizer=l1_l2(\n",
    "            l1={{uniform(0, 0.1)}}, \n",
    "            l2={{uniform(0, 0.1)}})))\n",
    "    model.add(Activation({{choice(['sigmoid', 'relu', 'tanh'])}}))\n",
    "    if {{choice(['bn1_yes', 'bn1_no'])}} == 'bn1_yes':\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(\n",
    "        {{choice([128, 192, 256, 512, 1024, 2048])}},\n",
    "        kernel_initializer={{choice(['zeros', 'glorot_normal', 'he_normal'])}},           \n",
    "        bias_initializer={{choice(['zeros', 'glorot_normal', 'he_normal'])}},          \n",
    "        kernel_regularizer=l1_l2(\n",
    "            l1={{uniform(0, 0.1)}}, \n",
    "            l2={{uniform(0, 0.1)}}),\n",
    "        bias_regularizer=l1_l2(\n",
    "            l1={{uniform(0, 0.1)}}, \n",
    "            l2={{uniform(0, 0.1)}}),\n",
    "        activity_regularizer=l1_l2(\n",
    "            l1={{uniform(0, 0.1)}}, \n",
    "            l2={{uniform(0, 0.1)}})))\n",
    "    model.add(Activation({{choice(['sigmoid', 'relu', 'tanh'])}}))\n",
    "    if {{choice(['bn2_yes', 'bn2_no'])}} == 'bn2_yes':\n",
    "        model.add(BatchNormalization())\n",
    "    model.add(Dropout({{uniform(0, 1)}}))\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    adamax = keras.optimizers.Adamax(\n",
    "        learning_rate={{uniform(0.001, 0.1)}},\n",
    "        beta_1={{uniform(0.75, 1)}},\n",
    "        beta_2={{uniform(0.75, 1)}})\n",
    "        \n",
    "    model.compile(loss='mse', \n",
    "                  metrics=['mae'],\n",
    "                  optimizer=adamax)\n",
    "\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=3)\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size={{choice([16, 32, 64, 128, 256, 512])}},\n",
    "              epochs=500,\n",
    "              verbose=2,\n",
    "              validation_data=(X_test, y_test),\n",
    "#               validation_split=0.15,\n",
    "              callbacks=[es])\n",
    "\n",
    "    mse, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "    print('Test mse:', mse)\n",
    "    return {'loss': mse, 'status': STATUS_OK, 'model': model}\n",
    "\n",
    "# def model(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "#     model = tf.keras.Sequential()\n",
    "#     model.add(Dense(\n",
    "#         256,\n",
    "#         activation='sigmoid', \n",
    "#         input_shape=[X_train.shape[1]],\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.1)))        \n",
    "#     model.add(Dropout(0.005))\n",
    "#     model.add(Dense(\n",
    "#         256,\n",
    "#         activation='sigmoid',\n",
    "#         kernel_initializer='glorot_normal', \n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=0.001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01)))\n",
    "#     model.add(Dropout(0.5))\n",
    "        \n",
    "#     model.add(Dense(\n",
    "#         1,\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         activation='linear'\n",
    "#     ))\n",
    "    \n",
    "#     adamax = keras.optimizers.Adamax(\n",
    "#         learning_rate=0.001,\n",
    "#         beta_1={{uniform(0.9, 1)}},\n",
    "#         beta_2={{uniform(0.9, 1)}})\n",
    "        \n",
    "#     model.compile(loss='mse', \n",
    "#                   metrics=['mae'],\n",
    "#                   optimizer=adamax)\n",
    "\n",
    "#     es = EarlyStopping(\n",
    "#         monitor='val_loss', \n",
    "#         mode='min', \n",
    "#         verbose=1, \n",
    "#         patience=20)\n",
    "\n",
    "#     model.fit(X_train, y_train,\n",
    "#               batch_size=256,\n",
    "#               epochs=500,\n",
    "#               verbose=2,\n",
    "#               shuffle=True,\n",
    "#               validation_data=(X_test, y_test),\n",
    "# #               validation_split=0.15,\n",
    "#               callbacks=[es])\n",
    "    \n",
    "#     mse, mae = model.evaluate(X_test, y_test, verbose=1)\n",
    "#     print('Test mse:', mse)\n",
    "#     return {'loss': mse, 'status': STATUS_OK, 'model': model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = None\n",
    "best_model = None\n",
    "space = None\n",
    "trials=Trials()\n",
    "best_run, best_model, space = optim.minimize(model=model,\n",
    "                                      data=get_data,\n",
    "                                      algo=tpe.suggest,\n",
    "                                      max_evals=500,\n",
    "                                      trials=trials,\n",
    "                                      notebook_name='keras',\n",
    "                                      eval_space=True,\n",
    "                                      return_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics(best_model, data, process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adamax = keras.optimizers.Adamax(learning_rate=0.001,beta_1=0.958,beta_2=0.987)\n",
    "\n",
    "def build_model():\n",
    "  model = tf.keras.Sequential([\n",
    "    Dense(\n",
    "        256, \n",
    "        activation='sigmoid', \n",
    "        input_shape=[len(data['X_train'].keys())],\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001),\n",
    "        bias_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.1)\n",
    "    ),\n",
    "    Dropout(0.005),\n",
    "    Dense(\n",
    "        256, \n",
    "        activation='sigmoid',\n",
    "        kernel_initializer='glorot_normal',\n",
    "        kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=0.001),\n",
    "        bias_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "    ),\n",
    "    Dropout(0.5),  \n",
    "    Dense(\n",
    "        1,\n",
    "        kernel_initializer='glorot_normal',\n",
    "        activation='linear'\n",
    "    )\n",
    "  ])\n",
    "\n",
    "  model.compile(loss='mse',\n",
    "                optimizer=adamax,\n",
    "                metrics=['mae', 'mse'])\n",
    "  return model\n",
    "\n",
    "model3 = build_model()\n",
    "# model3.summary()\n",
    "\n",
    "es = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    mode='min', \n",
    "    verbose=1, \n",
    "    patience=20)\n",
    "\n",
    "history = model3.fit(\n",
    "    data['X_train'], data['y_train'],\n",
    "    epochs=10000, \n",
    "    validation_data=(data['X_test'], data['y_test']),\n",
    "    verbose=0,\n",
    "    batch_size=256,\n",
    "    shuffle=True,\n",
    "    callbacks=[tfdocs.modeling.EpochDots(), es])\n",
    "#     callbacks=[es])\n",
    "output_metrics(model3, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics(model3, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mape: 701.1628654644568\n",
    "mae: 41859822.98122131\n",
    "rmse: 92053624.95453802\n",
    "adj_r2: 0.44188860358712956"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_metrics(model3, data, process, with_val=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "pyplot.plot(history.history['loss'], label='train loss')\n",
    "pyplot.plot(history.history['val_loss'], label='test loss')\n",
    "pyplot.plot(history.history['mae'], label='train mae')\n",
    "pyplot.plot(history.history['val_mae'], label='test mae')\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu wo initialization\n",
    "\n",
    "# mape: 6.6013761902604395\n",
    "# mae: 47785029.21390841\n",
    "# rmse: 117085745.28254725\n",
    "# adj_r2: 0.2683608578800275"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid\n",
    "\n",
    "# mape: 6.921294519771364\n",
    "# mae: 46309187.6036963\n",
    "# rmse: 106573312.92760979\n",
    "# adj_r2: 0.39384203627853387"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sigmoid initialization glorot\n",
    "\n",
    "# mape: 5.891309178898037\n",
    "# mae: 45178191.806934245\n",
    "# rmse: 105538830.70659108\n",
    "# adj_r2: 0.40555259042845115"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relu initialization he-normal\n",
    "\n",
    "# mape: 11.22876428439224\n",
    "# mae: 49985922.89351641\n",
    "# rmse: 124054429.88760668\n",
    "# adj_r2: 0.1786780362328163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Dense': 1024,\n",
    "#  'Dense_1': 256,\n",
    "#  'Dense_2': 1024,\n",
    "#  'Dropout': 0.15106219282775246,\n",
    "#  'Dropout_1': 0.6763508226311498,\n",
    "#  'Dropout_2': 'two',\n",
    "#  'Dropout_3': 0.30987873188582754,\n",
    "#  'batch_size': 256,\n",
    "#  'choiceval': 'adam',\n",
    "#  'lr': 0.001,\n",
    "#  'lr_1': 0.01,\n",
    "#  'lr_2': 0.1}\n",
    "\n",
    "# mape: 14.735849474528678\n",
    "# mae: 42425257.59364493\n",
    "# rmse: 93314406.47801651\n",
    "# adj_r2: 0.5986823964694081"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Dense': 256,\n",
    "#  'Dense_1': 1024,\n",
    "#  'Dense_2': 1024,\n",
    "#  'Dropout': 0.2974750265433348,\n",
    "#  'Dropout_1': 0.39966419778642664,\n",
    "#  'Dropout_2': 'two',\n",
    "#  'Dropout_3': 0.526026582745637,\n",
    "#  'activation': 'sigmoid',\n",
    "#  'activation_1': 'sigmoid',\n",
    "#  'activation_2': 'sigmoid',\n",
    "#  'batch_size': 128,\n",
    "#  'epochs': 75,\n",
    "#  'kernel_initializer': <tensorflow.python.ops.init_ops_v2.VarianceScaling at 0x7f8b393ae710>,\n",
    "#  'kernel_initializer_1': <tensorflow.python.ops.init_ops_v2.GlorotUniform at 0x7f8b393f3610>,\n",
    "#  'kernel_initializer_2': <tensorflow.python.ops.init_ops_v2.VarianceScaling at 0x7f8b2aec9750>,\n",
    "#  'optimizer': 'adam'}\n",
    "\n",
    "# mape: 11.930947077248565\n",
    "# mae: 41936947.39355901\n",
    "# rmse: 94227681.26787286\n",
    "# adj_r2: 0.5117092256416581"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Dense': 256,\n",
    "#  'Dense_1': 256,\n",
    "#  'Dense_2': 1024,\n",
    "#  'Dropout': 0.12206225819915595,\n",
    "#  'Dropout_1': 0.197064731659927,\n",
    "#  'Dropout_2': 'two',\n",
    "#  'Dropout_3': 0.00029842311592569865,\n",
    "#  'activation': 'sigmoid',\n",
    "#  'activation_1': 'sigmoid',\n",
    "#  'activation_2': 'relu',\n",
    "#  'batch_size': 128,\n",
    "#  'epochs': 25,\n",
    "#  'kernel_initializer': 'glorot_normal',\n",
    "#  'kernel_initializer_1': 'glorot_normal',\n",
    "#  'kernel_initializer_2': 'glorot_uniform',\n",
    "#  'optimizer': 'adam'}\n",
    "\n",
    "# mape: 6.224157960480135\n",
    "# mae: 43643280.72317956\n",
    "# rmse: 99891850.23558912\n",
    "# adj_r2: 0.5618906445214958"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'Dense': 512,\n",
    "#  'Dense_1': 1024,\n",
    "#  'Dense_2': 1024,\n",
    "#  'Dropout': 0.003154114487844459,\n",
    "#  'Dropout_1': 0.49453988643233404,\n",
    "#  'Dropout_2': 'two',\n",
    "#  'Dropout_3': 0.6057457357612506,\n",
    "#  'activation': 'sigmoid',\n",
    "#  'activation_1': 'sigmoid',\n",
    "#  'activation_2': 'sigmoid',\n",
    "#  'batch_size': 512,\n",
    "#  'epochs': 100,\n",
    "#  'kernel_initializer': 'glorot_normal',\n",
    "#  'kernel_initializer_1': 'he_uniform',\n",
    "#  'kernel_initializer_2': 'glorot_normal',\n",
    "#  'optimizer': 'adam'}\n",
    "\n",
    "\n",
    "# mape: 5.984611554637821\n",
    "# mae: 43830607.21558445\n",
    "# rmse: 99151995.47327316\n",
    "# adj_r2: 0.5683563756398305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "#   model = tf.keras.Sequential([\n",
    "#     Dense(\n",
    "#         512, \n",
    "#         activation='sigmoid', \n",
    "#         input_shape=[len(data['X_train'].keys())],\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.1)\n",
    "#     ),\n",
    "#     Dropout(0.005),\n",
    "#     Dense(\n",
    "#         1024, \n",
    "#         activation='sigmoid',\n",
    "#         kernel_initializer='he_uniform',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=0.001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "#         activity_regularizer=l1_l2(l1=0, l2=0.00001)\n",
    "#     ),\n",
    "#     Dropout(0.7), \n",
    "#     Dense(1)\n",
    "#   ])\n",
    "\n",
    "#   model.compile(loss='mean_squared_error',\n",
    "#                 optimizer='adam',\n",
    "#                 metrics=['mae', 'mean_squared_error'])\n",
    "#   return model\n",
    "\n",
    "# model3 = build_model()\n",
    "# model3.summary()\n",
    "\n",
    "# mape: 8.161095980091112\n",
    "# mae: 35879531.109611064\n",
    "# rmse: 87476559.6580578\n",
    "# adj_r2: 0.5608763424966564"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def build_model():\n",
    "#   model = tf.keras.Sequential([\n",
    "#     Dense(\n",
    "#         256, \n",
    "#         activation='sigmoid', \n",
    "#         input_shape=[len(data['X_train'].keys())],\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.1)\n",
    "#     ),\n",
    "#     Dropout(0.005),\n",
    "#     Dense(\n",
    "#         256, \n",
    "#         activation='sigmoid',\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=0.001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "#     ),\n",
    "#     Dropout(0.5),  \n",
    "#     Dense(1)\n",
    "#   ])\n",
    "\n",
    "#   model.compile(loss='mae',\n",
    "#                 optimizer='adam',\n",
    "#                 metrics=['mae', 'mean_squared_error'])\n",
    "#   return model\n",
    "\n",
    "# model3 = build_model()\n",
    "# # model3.summary()\n",
    "\n",
    "# es = EarlyStopping(\n",
    "#     monitor='val_loss', \n",
    "#     mode='min', \n",
    "#     verbose=1, \n",
    "#     patience=50)\n",
    "\n",
    "# history = model3.fit(\n",
    "#     data['X_train'], data['y_train'],\n",
    "#     epochs=10000, \n",
    "#     validation_data=(data['X_test'], data['y_test']),\n",
    "#     verbose=1,\n",
    "#     batch_size=256,\n",
    "#     shuffle=True,\n",
    "# #     callbacks=[tfdocs.modeling.EpochDots(), es])\n",
    "#     callbacks=[es])\n",
    "\n",
    "\n",
    "# mape: 5.821131932635737\n",
    "# mae: 42307258.2010261\n",
    "# rmse: 95281127.2382761\n",
    "# adj_r2: 0.4884981254695354"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skew x -> skew y\n",
    "\n",
    "# adamax = keras.optimizers.Adamax(learning_rate=0.001,beta_1=0.95,beta_2=0.999)\n",
    "\n",
    "# def build_model():\n",
    "#   model = tf.keras.Sequential([\n",
    "#     Dense(\n",
    "#         256, \n",
    "#         activation='sigmoid', \n",
    "#         input_shape=[len(data['X_train'].keys())],\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.1)\n",
    "#     ),\n",
    "#     Dropout(0.005),\n",
    "#     Dense(\n",
    "#         256, \n",
    "#         activation='sigmoid',\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=0.001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "#     ),\n",
    "#     Dropout(0.5),  \n",
    "#     Dense(\n",
    "#         1,\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         activation='linear'\n",
    "#     )\n",
    "#   ])\n",
    "\n",
    "#   model.compile(loss='mse',\n",
    "#                 optimizer=adamax,\n",
    "#                 metrics=['mae', 'mse'])\n",
    "#   return model\n",
    "\n",
    "# model3 = build_model()\n",
    "# # model3.summary()\n",
    "\n",
    "# es = EarlyStopping(\n",
    "#     monitor='val_loss', \n",
    "#     mode='min', \n",
    "#     verbose=1, \n",
    "#     patience=20)\n",
    "\n",
    "# history = model3.fit(\n",
    "#     data['X_train'], data['y_train'],\n",
    "#     epochs=1000, \n",
    "#     validation_data=(data['X_test'], data['y_test']),\n",
    "#     verbose=0,\n",
    "#     batch_size=256,\n",
    "#     shuffle=True,\n",
    "# #     callbacks=[tfdocs.modeling.EpochDots(), es])\n",
    "#     callbacks=[es])\n",
    "\n",
    "# mape: 6.094501890528896\n",
    "# mae: 40504269.625439286\n",
    "# rmse: 87054173.39806816\n",
    "# adj_r2: 0.5730149701057303"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # skew x -> skew y\n",
    "\n",
    "# import lightgbm as lgb\n",
    "# mod_lgb = lgb.LGBMRegressor(\n",
    "#     objective='regression',\n",
    "#     num_leaves=34,\n",
    "#     learning_rate=0.001, \n",
    "#     n_estimators=7500,\n",
    "#     max_bin=192,\n",
    "#     max_depth=0,\n",
    "#     min_child_samples=160,\n",
    "#     min_child_weight=0.001,\n",
    "#     bagging_fraction=0.98,\n",
    "#     bagging_freq=15, \n",
    "#     feature_fraction=0.77,\n",
    "#     feature_fraction_seed=9,\n",
    "#     bagging_seed=9,\n",
    "#     min_data_in_leaf=1, \n",
    "#     min_sum_hessian_in_leaf=50,\n",
    "#     colsample_bytree=0.87,\n",
    "#     reg_alpha=0.18,\n",
    "#     reg_lambda=30,\n",
    "#     subsample=0.39,\n",
    "#     tree_learner='data',\n",
    "# )\n",
    "# mod_lgb.fit(\n",
    "#     data['X_train'].values, \n",
    "#     data['y_train'],\n",
    "#     verbose=0,\n",
    "#     eval_metric='mse',\n",
    "#     eval_set=[(data['X_test'], data['y_test'])],\n",
    "#     early_stopping_rounds=25\n",
    "# )\n",
    "# output_metrics(mod_lgb, data, process, with_val=True)\n",
    "\n",
    "# mape: 8.358551244895033\n",
    "# mae: 40639301.811736666\n",
    "# rmse: 83874926.05518548\n",
    "# adj_r2: 0.49830307531562434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adamax = keras.optimizers.Adamax(learning_rate=0.001,beta_1=0.958,beta_2=0.987)\n",
    "\n",
    "# def build_model():\n",
    "#   model = tf.keras.Sequential([\n",
    "#     Dense(\n",
    "#         256, \n",
    "#         activation='sigmoid', \n",
    "#         input_shape=[len(data['X_train'].keys())],\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0.0001, l2=0.0001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.001, l2=0.1)\n",
    "#     ),\n",
    "#     Dropout(0.005),\n",
    "#     Dense(\n",
    "#         256, \n",
    "#         activation='sigmoid',\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         kernel_regularizer=keras.regularizers.l1_l2(l1=0, l2=0.001),\n",
    "#         bias_regularizer=keras.regularizers.l1_l2(l1=0.01, l2=0.01),\n",
    "#     ),\n",
    "#     Dropout(0.5),  \n",
    "#     Dense(\n",
    "#         1,\n",
    "#         kernel_initializer='glorot_normal',\n",
    "#         activation='linear'\n",
    "#     )\n",
    "#   ])\n",
    "\n",
    "#   model.compile(loss='mse',\n",
    "#                 optimizer=adamax,\n",
    "#                 metrics=['mae', 'mse'])\n",
    "#   return model\n",
    "\n",
    "# model3 = build_model()\n",
    "# # model3.summary()\n",
    "\n",
    "# es = EarlyStopping(\n",
    "#     monitor='val_loss', \n",
    "#     mode='min', \n",
    "#     verbose=1, \n",
    "#     patience=20)\n",
    "\n",
    "# history = model3.fit(\n",
    "#     data['X_train'], data['y_train'],\n",
    "#     epochs=10000, \n",
    "#     validation_data=(data['X_test'], data['y_test']),\n",
    "#     verbose=0,\n",
    "#     batch_size=256,\n",
    "#     shuffle=True,\n",
    "#     callbacks=[tfdocs.modeling.EpochDots(), es])\n",
    "# #     callbacks=[es])\n",
    "\n",
    "\n",
    "# mape: 715.0661530743256\n",
    "# mae: 41755141.579565056\n",
    "# rmse: 91255901.0566581\n",
    "# adj_r2: 0.45151972233459114"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "# mod_lgb = lgb.LGBMRegressor(\n",
    "#     objective='regression',\n",
    "#     num_leaves=34,\n",
    "#     learning_rate=0.001, \n",
    "#     n_estimators=7500,\n",
    "#     max_bin=192,\n",
    "#     max_depth=0,\n",
    "#     min_child_samples=160,\n",
    "#     min_child_weight=0.001,\n",
    "#     bagging_fraction=0.98,\n",
    "#     bagging_freq=15, \n",
    "#     feature_fraction=0.77,\n",
    "#     metric='l2',\n",
    "#     bagging_seed=9,\n",
    "#     min_data_in_leaf=1, \n",
    "#     min_sum_hessian_in_leaf=50,\n",
    "#     colsample_bytree=0.87,\n",
    "#     reg_alpha=0.18,\n",
    "#     reg_lambda=30,\n",
    "#     subsample=0.39,\n",
    "#     tree_learner='data',\n",
    "# )\n",
    "# mod_lgb.fit(\n",
    "#     data['X_train'].values, \n",
    "#     data['y_train'],\n",
    "#     verbose=2,\n",
    "#     eval_metric='mse',\n",
    "#     eval_set=[(data['X_test'], data['y_test'])],\n",
    "#     early_stopping_rounds=50\n",
    "# )\n",
    "# output_metrics(mod_lgb, data, process, with_val=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}